Dependencies: [[Vector]]
## Definition
A $n\times m$ matrix is a two-dimensional array of real numbers with $n$ rows and $m$ columns:$$A = \begin{pmatrix} A_{11} & A_{12} & \dots & A_{1m}\\ A_{21} & A_{22} & \dots & A_{2m}\\\vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \dots & A_{nm}\end{pmatrix} \qquad \text{where }A_{ij} \in\mathbb R \text{ for } 1\leq i \leq n,\; 1\leq j\leq m$$An $n\times m$ matrix can right-multiply $m$-dimensional vectors to produce $n$-dimensional vectors as follows: $$A\vec x = \begin{pmatrix} A_{11} & A_{12} & \dots & A_{1m}\\ A_{21} & A_{22} & \dots & A_{2m}\\\vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \dots & A_{nm}\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\\vdots\\ x_m\end{pmatrix} = \begin{pmatrix} A_{11}x_1 + A_{12}x_2 + \dots + A_{1m} x_m \\ A_{21}x_1 + A_{22}x_2 + \dots + A_{2m}x_m \\\vdots\\ A_{n1}x_1 + x_{n2}x_2 + \dots + A_{nm}x_m\end{pmatrix}$$An $n$-dimensional vector can be thought of as a $n\times 1$ matrix. We can define the $n\times k$ matrix-matrix product of an $n\times m$ matrix $A$ and a $m\times k$ matrix B as a column-wise extension of the matrix-vector product. It is not shown here for brevity. We can also define the addition of two matrices as element-wise addition. Finally, we can define the scalar multiplication of a matrix as the element-wise scalar multiplication. These definitions extend naturally to vectors.

All linear maps from $\mathbb R^m$, the space of $m$-dimensional vectors, to $\mathbb R^n$, the space of $n$-dimensional vectors, can be expressed as a $n\times m$ matrix. (A linear map is a map $A$ that commutes with vector addition and scalar multiplication: $A (a\vec x + b\vec y) = a A\vec x + b A\vec y$.) Two linear maps $A$ and $B$ can be composed via matrix multiplication: $A(B\vec x) = (AB)\vec x$.

We can identify an element of the matrix $A$ above by writing $A_{ij}$, as shown above. We can identify the $i$th column or row of the matrix above by writing $A_{: i}$ or $A_{i:}$, using the colon to indicate "all". 

The **transpose** of an $n\times m$ matrix is an $m\times n$ matrix whose components are given by $A_{ji}$:$$A^T = \begin{pmatrix} A_{11} & A_{21} & \dots & A_{n1}\\ A_{12} & A_{22} & \dots & A_{n2}\\\vdots & \vdots & \ddots & \vdots \\ A_{1m} & A_{2m} & \dots & A_{nm}\end{pmatrix} \qquad \text{where }A_{ij} \in\mathbb R \text{ for } 1\leq i \leq n,\; 1\leq j\leq m$$Notice that the transpose can be extended to vectors: the transpose of a vector is a "row vector". The transpose satisfies $(AB)^T = B^TA^T$ for any matrices or vectors $A, B$. Note that we can form the **dot product** $\vec x \cdot \vec y = x_1 y_1 + \dots + x_n y_n$ of two $n$-dimensional vectors as a matrix multiplication using the transpose: $\vec x \cdot \vec y = \vec x^T\vec y$.

One special matrix is the $n$-dimensional **identity** $I_n$, an $n\times n$ matrix defined by $(I_n)_{ii} = 1$ for $1\leq i\leq n$ and $(I_n)_{ij} = 0$ for $i\neq j$. This matrix does nothing under matrix multiplication: $I_n A = A$ for an $n\times m$ matrix $A$, and $AI_n = A$ for a $m\times n$ matrix $A$. Notice this applies to column and row vectors too, by choosing $m = 1$.