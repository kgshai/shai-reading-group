\Paper: [Swamy et al. (2021)](https://proceedings.mlr.press/v139/swamy21a/swamy21a.pdf)

- Formulate Imitation Learning (IL) / Inverse RL (IRL) as a game between policies and possible reward functions
	- Existing IL/IRL techniques can be interpreted as moment-matching for different moments
	- Three types of moments: rewards, on-policy Q-values, off-policy Q-values
	- Off policy vs on policy refers to whether trajectories used to calculate moments come from the dataset or new policy rollouts, respectively
	- Reward requires access to env, off-Q requires nothing but is error prone, on-Q requires access to env and a queryable expert but has provable guarantees

Assume $r(s, a)\in[-1,1]$ and horizon of $T$, initial state distribution $P_0$.
Imitation gap: $J(\pi_E) - J(\pi)$
Class of rewards: $\mathcal F_r = \{r : S\times A \to [-1, 1]\}$
Class of on-policy Qs: $\mathcal F_Q = \{Q: S\times A\to [-T, T]\}$
Class of off-policy Qs: $\mathcal F_{Q_E} = \{Q: S\times A\to [-\bar Q, \bar Q] \text{ and }Q\text{ matches the dataset}\}$
Use $\bar Q$ because $\pi_E$ determines the off-policy Qs allowed, these may not reach $\pm T$
$H$-recoverability: $\forall s\in S\quad \forall a\in A \quad \forall f\in \mathcal F_{Q_E}\quad \lvert f(s, a) - \mathbb E_{a'\sim\pi_E(s)} f(s, a')\rvert < H$, i.e. no action is $H$ worse in terms of reward than the expected reward from $\pi_E$. An upper bound on how long it takes the expert to recover from a single mistake (non-expert action).

Now, note that in terms of reward:
$$\begin{align*}J(\pi_E) - J(\pi)&=\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T r(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T r(s_t, a_t)

\\&\leq\sup_{f\in\mathcal F_r}\left[\mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T f(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T f(s_t, a_t)\right]


\end{align*}$$
For the on-policy and off-policy Q-value moment matching formulas, we use the performance difference lemma, which states that:
$$V^{\pi}(s) - V^{\pi'}(s) = T\mathop{\mathbb E}_{s'\sim P^{\pi}(s)} \mathop{\mathbb E}_{a'\sim\pi(s)} A^{\pi'}(s', a')$$
In other words, we can get the difference in return between policies $\pi$ and $\pi'$ by averaging at the action values of $\pi'$ in states encountered by $\pi$. (It also implies we can evaluate this difference by averaging the action values of $\pi$ in states encountered by $\pi'$.)

In terms of the Q-values $f\in\mathcal F_{Q}$:
$$\begin{align*}J(\pi_E) - J(\pi)&=\mathop{\mathbb E}_{s\sim P_0}\left[V^{\pi_E}(s) - V^\pi(s)\right]

\\&=\mathop{\mathbb E}_{s\sim P_0}\left[T\mathop{\mathbb E}_{s'\sim P^{\pi_E}(s)}\mathop{\mathbb E}_{a'\sim \pi_E(s')}A^{\pi}(s', a')\right]

\\&=\mathop{\mathbb E}_{\tau\sim\pi_E}\left[\sum_{t=1}^TA^{\pi}(s_t, a_t)\right]

\\&=\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left(Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)\right)

\\&=\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[Q^{\pi}(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi}Q^{\pi}(s_t, a')\right]

\\&\leq\sup_{f\in\mathcal F_{Q}}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[\mathop{\mathbb E}_{a'\sim\pi}f(s_t, a') - f(s_t, a_t) \right]

\end{align*}$$
In terms of the on-policy Q-values $f\in\mathcal F_{Q_E}$:$$\begin{align*}J(\pi_E) - J(\pi)&=\mathop{\mathbb E}_{s\sim P_0}\left[V^{\pi_E}(s) - V^\pi(s)\right]

\\&=\mathop{\mathbb E}_{s\sim P_0}\left[-T\mathop{\mathbb E}_{s'\sim P^{\pi}(s)}\mathop{\mathbb E}_{a'\sim \pi(s')}A^{\pi_E}(s', a')\right]

\\&=\mathop{\mathbb E}_{\tau\sim\pi}\left[\sum_{t=1}^T-A^{\pi_E}(s_t, a_t)\right]

\\&=\mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T\left(V^{\pi_E}(s_t) - Q^{\pi_E}(s_t, a_t)\right)

\\&=\mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T\left[\mathop{\mathbb E}_{a'\sim\pi_E}Q^{\pi_E}(s_t, a') - Q^{\pi_E}(s_t, a_t)\right]

\\&\leq\sup_{f\in\mathcal F_{Q_E}}\mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T\left[f(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi_E}f(s_t, a')\right]

\end{align*}$$We now need a united function class that bridges $\mathcal F_r$, $\mathcal F_Q$, and $\mathcal F_{Q_E}$, accounting for the fact that functions in $\mathcal F_r$ take values in $[-1, 1]$ while the other two take values in $[-T, T]$. We let $\mathcal F$ consist of convex combinations of functions in $\mathcal F_r / 2$, $\mathcal{F}_Q / 2T$, or $\mathcal{F}_{Q_E} / 2T$. The scaling factors are introduced to ensure that the set of possible minimax objective values for for $f\in\mathcal F$ are all measure-1:$$\begin{align*}U_1(\pi, f) &= \frac{1}{T}\left[\mathop{\mathbb E}_{\tau\sim\pi} \sum_{t=1}^T f(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\pi_E} \sum_{t=1}^T f(s_t, a_t)\right]\\ U_2(\pi, f) &= \frac{1}{T}\left[\mathop{\mathbb E}_{\substack{\tau\sim\pi_E\\a\sim \pi(s_t)}} \sum_{t=1}^T f(s_t, a) - \mathop{\mathbb E}_{\tau\sim\pi_E} \sum_{t=1}^T f(s_t, a_t)\right]\\ U_3(\pi, f) &= \frac{1}{T}\left[\mathop{\mathbb E}_{\tau\sim\pi} \sum_{t=1}^T f(s_t, a_t) - \mathop{\mathbb E}_{\substack{\tau\sim\pi\\a\sim \pi_E(s_t)}} \sum_{t=1}^T f(s_t, a)\right]\end{align*}$$The minimax problems are $$\min_{\pi\in\Pi}\max_{f\in\mathcal F} U_i(\pi, f)$$Suppose we have an oracle $\Phi\{\varepsilon\}(\cdot)$ which outputs a $\varepsilon$-approximate equilibrium $(\hat\pi, \hat f)$, which definitionally must satisfy:$$\sup_{f\in\mathcal F} U_i(\hat\pi, f) - \frac{\varepsilon}{2} \leq U_i(\hat \pi, \hat f) \leq \inf_{\pi\in\Pi}U_i(\pi, \hat f) + \frac{\varepsilon}{2}$$Meaning that $\hat\pi$ and $\hat f$ achieve utility within $\frac{\varepsilon}{2}$ of the best responses for each player in the minimax game. Notice that $U_i(\pi_E, f) = 0$ for all $f\in\mathcal F$, so assuming $\pi_E\in\Pi$, we have $\inf_{\pi\in\Pi} U_i(\pi, f) \leq 0$ for any $f \in\mathcal F$. Thus, the $\varepsilon$-approximate equilibrium $(\hat\pi, \hat f)$ satisfies 
$$\sup_{f\in\mathcal F} U_i(\hat\pi, f) - \frac{\varepsilon}{2} \leq U_i(\hat \pi, \hat f) \leq \inf_{\pi\in\Pi}U_i(\pi, \hat f) + \frac{\varepsilon}{2} \leq \frac{\varepsilon}{2} \quad\Longrightarrow \quad \sup_{f\in\mathcal F} U_i(\hat\pi, f) \leq \varepsilon$$
This fact will help provide upper bounds on the imitation gap $J(\pi_E) - J(\hat\pi)$ for an $\varepsilon$-approximate equilibrium policy $\hat\pi$. However, we would like to know whether tighter upper bounds are possible. One way to establish lower bounds for the imitation gap is to evaluate it on some example MDPs. We present some MDPs that we use to establish imitation gap lower bounds. On the left is LOOP, an MDP that rewards spending time in state $s_1$ and penalizes spending time in state $s_2$. On the right is CLIFF, a folklore MDP in which the learner must avoid "falling off the cliff" into the absorbing state $s_x$. The color of the arrows generally indicate which state-actions will be assigned positive/negative reward, but we define the precise reward functions for these MDPs later.![[LoopCliff.png]]
We are now ready to provide performance bounds for the reward, off-policy Q, and on-policy Q variants of moment matching, assuming we can obtain $\varepsilon$-approximate equilibriums to the relevant minimax problems. 
**Theorem 1 (Reward Imitation Gap Bound)**: If $\hat\pi\leftarrow \Phi\{\varepsilon\}(U_1)$, then we have $\Omega(\varepsilon T) \leq J(\pi_E) - J(\hat\pi) \leq O(\varepsilon T)$.
*Proof*: Simply combine the following two lemmas.
**Lemma 1 (Reward Imitation Gap Upper Bound)**: If $\hat\pi\leftarrow \Phi\{\varepsilon\}(U_1)$, then $J(\pi_E) - J(\hat\pi) \leq O(\varepsilon T)$.
*Proof*: We use the minimax expression for the imitation gap, keeping in mind that our function search space $\mathcal F$ contains $f/2$ for each $f\in\mathcal F_r$:$$\begin{align*}J(\pi_E) - J(\hat\pi)&\leq\sup_{f\in\mathcal F_r}\left[\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T f(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\hat\pi}\sum_{t=1}^T f(s_t, a_t)\right]\\&\leq\sup_{f\in\mathcal F}\left[\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T 2f(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\hat\pi}\sum_{t=1}^T 2f(s_t, a_t)\right]\\&\leq2T\sup_{f\in\mathcal F} U_1(\hat\pi, f) \leq 2T\varepsilon
\end{align*}$$The second step involves replacing the search over $f\in\mathcal F_r$ with a search over $f/2\in\mathcal F$, and the last step uses the fact that $\hat\pi$ is part of an $\varepsilon$-approximate equilibrium.
**Lemma 2 (Reward Imitation Gap Lower Bound)**: There exists an MDP, expert policy $\pi_E$, and $\varepsilon$-approximate equilibrium $\hat\pi, \hat f\leftarrow \Phi\{\varepsilon\}(U_1)$ such that $J(\pi_E) - J(\hat\pi) \geq \Omega(\varepsilon T)$.
*Proof*: We define the MDP, $\pi_E$, and $\hat \pi$. We then confirm that $J(\pi_E) - J(\hat\pi) \geq\Omega(\varepsilon T)$, and we find $\hat f$ such that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, showing that $\hat\pi$ could feasibly be returned by the oracle $\Phi$. Consider CLIFF with the reward function $r(s, a) = -\mathbb{1}(s = s_x) -\mathbb{1}(a = a_2)$, so we receive negative reward for falling off the cliff (second term) and negative reward every timestep after we fall (first term). Suppose $\pi_E$ always takes action $a_1$ and never falls off the cliff (it is perfect). Suppose $\hat\pi$ takes action $a_2$ with probability $\varepsilon / 2$ in state $s_0$, and otherwise always takes $a_1$. First, note that $\pi_E$ never falls off the cliff, so $J(\pi_E) = 0$. On the other hand, $\hat\pi$ falls off the cliff in state $s_0$ with probability $\varepsilon / 2$, and stays away with probability $1 - \varepsilon / 2$. In the former case, it obtains reward $T$; in the latter case, it obtains reward $0$. Thus $J(\pi) = -\varepsilon T /2$, and the imitation gap is $J(\hat\pi) - J(\pi_E) = \varepsilon T / 2 \geq\Omega(\varepsilon T)$, as desired. Now, let $\hat f = -r-1/2$: when translated from $\mathcal F$ to $\mathcal F_r$, it assigns the maximum reward of $1$ when falling off the cliff and all timesteps afterwards, and assigns the minimum reward of $-1$ otherwise. To see that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, we must evaluate the three expressions $\sup_{f\in\mathcal F} U_1(\hat\pi, f)$, $U_1(\hat\pi, \hat f)$, and $\inf_{\pi\in\Pi} U_1(\pi, \hat f)$, and show they satisfy the inequality. We first notice that since $f\in\mathcal F$ are bounded to $[-1, 1]$, $\hat f$ is the function which maximally rewards $\hat\pi$ and penalizes $\pi_E$, meaning the supremum in the first expression corresponds to $f = \hat f$:$$\begin{align*}\sup_{f\in\mathcal F} U_1(\hat\pi, f) = U_1(\hat\pi, \hat f) &= \frac{1}{T}\left[\mathop{\mathbb E}_{\tau\sim\pi} \sum_{t=1}^T \hat f(s_t, a_t) - \mathop{\mathbb E}_{\tau\sim\pi_E} \sum_{t=1}^T \hat f(s_t, a_t)\right]\\&=\frac{1}{T}\left[\frac{\varepsilon}{2} \left(\frac{T}{2}\right) + \left(1 - \frac{\varepsilon}{2}\right)\left(-\frac{T}{2}\right) - \left(-\frac{T}{2}\right)\right] = \frac{\varepsilon}{2}\end{align*}$$Lastly, the policy $\pi$ which minimizes $U_1(\pi, \hat f)$ is the expert policy $\pi_E$ which is penalized maximally at every timestep for staying away from the cliff. Thus, the two expectations cancel, and $\inf_{\pi\in\Pi} U_1(\pi, \hat f) = 0$; this makes sense, as according to $\hat f$, any policy is better than $\pi_E$, which never falls off the cliff. Putting all this together, the inequality defining an $\varepsilon$-approximate equilibrium is $0\leq \varepsilon / 2\leq \varepsilon / 2$, which is clearly true, and thus $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium.
**Theorem 2 (Off-policy Q Imitation Gap Bound)**: If $\hat\pi\leftarrow \Phi\{\varepsilon\}(U_2)$, then we have $\Omega(\varepsilon T^2) \leq J(\pi_E) - J(\hat\pi) \leq O(\varepsilon T^2)$.
*Proof*: Simply combine the following two lemmas.
**Lemma 3 (Off-policy Q Imitation Gap Upper Bound)**: If $\hat\pi \leftarrow \Phi\{\varepsilon\}(U_2)$, then $J(\pi_E) - J(\hat\pi) \leq O(\varepsilon T^2)$.
*Proof*: We use the minimax expression for the imitation gap, keeping in mind that our function search space $\mathcal F$ contains $f / 2T$ for each $f\in\mathcal F_Q$. $$\begin{align*}J(\pi_E) - J(\hat \pi) &\leq \sup_{f\in\mathcal F_{Q}}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[f(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi}f(s_t, a')\right]\\& \leq \sup_{f\in\mathcal F}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[2Tf(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi}2Tf(s_t, a')\right]\\&\leq 2T^2\sup_{f\in\mathcal F} U_2(\pi, f) \leq 2\varepsilon T^2\end{align*}$$As in the previous upper bound, the second step replaces a search over $f\in\mathcal F_Q$ with a search over $f / 2T \in\mathcal F$. The last step uses the fact that $\hat\pi$ is an $\varepsilon$-approximate equilibrium. 
**Lemma 4 (Off-policy Q Imitation Gap Lower Bound)**: There exists an MDP, expert policy $\pi_E$, and $\varepsilon$-approximate equilibrium $\hat\pi, \hat f \leftarrow \Phi\{\varepsilon\}(U_2)$ such that $J(\pi_E) - J(\hat\pi) \geq \Omega(\varepsilon T^2)$.
*Proof*: Again, we define the MDP, $\pi_E$, and $\hat\pi$. We then confirm that $J(\pi_E) - J(\pi) \geq \Omega(\varepsilon T)$, and we find $\hat f$ such that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, showing that $\hat\pi$ could feasibly be returned by the oracle $\Phi$. We again use CLIFF as our MDP, with the reward function $r(s, a) = -\mathbb 1(s = s_x) - \mathbb 1(a = a_2)$. We define $\pi_E$ as the policy that always takes action $a_1$, and never falls off the cliff. Finally, we define $\hat\pi$ as the policy that takes action $a_2$ with probability $\varepsilon T / 2$ in state $s_0$, and otherwise always takes action $a_1$. Again, $\pi_E$ never falls off the cliff, so $J(\pi_E) = 0$, and this time $\hat\pi$ obtains reward $-T$ with probability $\varepsilon T / 2$ and $0$ otherwise, yielding $J(\hat\pi) = -\varepsilon T^2 / 2$, and the imitation gap is $J(\pi_E) - J(\hat\pi) = \varepsilon T^2 / 2 \geq \Omega(\varepsilon T^2)$ as desired. Now, let $\hat f = -r-1/2$: when translated from $\mathcal F$ to $\mathcal F_Q$, it assigns value $-T$ to actions that do not fall off the cliff, and value $T$ to actions that fall off the cliff (or have already fallen). To see that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, we must evaluate the three expressions $\sup_{f\in\mathcal F} U_2(\hat\pi, f)$, $U_2(\hat\pi, \hat f)$, and $\inf_{\pi\in\Pi} U_2(\pi, \hat f)$, and show they satisfy the inequality. First note that $$U_2 (\hat\pi, f) = \frac{1}{T}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[\mathop{\mathbb E}_{a'\sim\hat\pi}f(s_t, a') - f(s_t, a_t) \right] = \frac{1}{T}\left[\mathop{\mathbb E}_{a'\sim\hat\pi} f(s_0, a') - f(s_0, a_1)\right]$$so for this MDP, $\pi_E$, and $\hat\pi$, the value of $U_2(\hat\pi, f)$ is determined by the values of $f$ at $s_0$. Note that $\hat f$ is the function which assigns maximum value to $\hat\pi$ (sometimes falling off) and minimum value to $\pi_E$ (never falling off), meaning the supremum $\sup_{f\in\mathcal F} U_2(\hat\pi, f)$ again corresponds to $f = \hat f$:$$\begin{align*}\sup_{f\in\mathcal F} U_2(\hat\pi, f) = U_2(\hat\pi, \hat f) &=  \frac{1}{T}\left[\mathop{\mathbb E}_{a'\sim\hat\pi} \hat f(s_0, a') - \hat f(s_0, a_1)\right]\\&=\frac{1}{T}\left[\left(\frac{\varepsilon T}{2}\right)\left(\frac{1}{2}\right) + \left(1 - \frac{\varepsilon T}{2}\right)\left(-\frac{1}{2}\right) - \left(-\frac{1}{2}\right)\right] = \frac{\varepsilon}{2}\end{align*}$$Lastly, the policy $\pi$ which minimizes $U_1(\pi, \hat f)$ is the expert policy $\pi_E$, which has the minimal possible value at every timestep since it stays away from the cliff with probability 1. Thus, the two terms in the expression for $U_2(\pi_E, \hat f)$ cancel, and $\inf_{\pi\in\Pi} U_1(\pi, \hat f) = 0$; this makes sense, as according to $\hat f$, any policy is better than $\pi_E$, which never falls off the cliff. Putting all this together, the inequality defining an $\varepsilon$-approximate equilibrium is $0\leq \varepsilon / 2\leq \varepsilon / 2$, which is clearly true, and thus $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium.
**Definition ($H$-recoverability)**: A pair $(\pi_E, \mathcal F_{Q_E})$ of an expert policy and set of expert Q-functions is said to be $H$-recoverable if $\forall s\in S \; \forall a\in A\; \forall f\in\mathcal F_{Q_E} \; \lvert f(s, a) - \mathop{\mathbb E}_{a'\sim\pi_E(s)}f(s, a')\rvert < H$. Thus, $H$ is an upper bound on the possible advantage that the expert can obtain under a Q-function in $\mathcal F_{Q_E}$. This bound becomes important to the following imitation gap bound since evaluating $U_3$ involves simulating the actions of the expert in novel states. Notice that this condition implies 
$$\begin{align*}\lvert U_3(\pi, f)\rvert &= \frac{1}{T}\left\lvert\mathop{\mathbb E}_{\tau\sim\pi} \sum_{t=1}^T f(s_t, a) - \mathop{\mathbb E}_{\substack{\tau\sim\pi\\a\sim \pi_E(s_t)}} \sum_{t=1}^T f(s_t, a_t)\right\rvert\\&\leq\frac{1}{T}\mathop{\mathbb E}_{\tau\sim\pi}\sum_{t=1}^T \left\lvert  f(s_t, a_t) - \mathop{\mathbb E}_{a\sim \pi_E(s_t)}f(s_t, a)\right\rvert \leq H\end{align*}$$
**Theorem 3 (On-policy Q Imitation Gap Bound)**: If $\hat\pi\leftarrow \Phi\{\varepsilon\}(U_3)$, then we have $\Omega(\varepsilon T) \leq J(\pi_E) - J(\hat\pi) \leq O(\varepsilon HT)$.
*Proof*: Simply combine the following two lemmas.
**Lemma 5 (On-policy Q Imitation Gap Upper Bound)**: If $\hat\pi \leftarrow \Phi\{\varepsilon\}(U_3)$, then $J(\pi_E) - J(\hat\pi) \leq O(\varepsilon HT)$.
*Proof*: We use the minimax expression for the imitation gap, keeping in mind that our function search space $\mathcal F$ contains $f / 2T$ for each $f\in\mathcal F_Q$. $$\begin{align*}J(\pi_E) - J(\hat \pi) &\leq \sup_{f\in\mathcal F_{Q}}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[f(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi}f(s_t, a')\right]\\& \leq \sup_{f\in\mathcal F}\mathop{\mathbb E}_{\tau\sim\pi_E}\sum_{t=1}^T\left[2Tf(s_t, a_t) - \mathop{\mathbb E}_{a'\sim\pi}2Tf(s_t, a')\right]\\&\leq 2T^2\sup_{f\in\mathcal F} U_3(\pi, f) \leq 2T^2\left(\frac{\varepsilon H}{2T}\right)= \varepsilon H T\end{align*}$$As in the previous upper bound, the second step replaces a search over $f\in\mathcal F_Q$ with a search over $f / 2T \in\mathcal F$. The last step uses the fact that $\hat\pi$ is an $\varepsilon$-approximate equilibrium, along with $H$-recoverability (how?).
**Lemma 6 (On-policy Q Imitation Gap Lower Bound)**: There exists an MDP, expert policy $\pi_E$, and $\varepsilon$-approximate equilibrium $\hat\pi, \hat f \leftarrow \Phi\{\varepsilon\}(U_2)$ such that $J(\pi_E) - J(\hat\pi) \geq \Omega(\varepsilon T)$.
*Proof*: The proof proceeds exactly as in the reward case. We define the MDP, $\pi_E$, and $\hat \pi$. We then confirm that $J(\pi_E) - J(\hat\pi) \geq\Omega(\varepsilon T)$, and we find $\hat f$ such that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, showing that $\hat\pi$ could feasibly be returned by the oracle $\Phi$. Consider CLIFF with the reward function $r(s, a) = -\mathbb{1}(s = s_x) -\mathbb{1}(a = a_2)$, so we receive negative reward for falling off the cliff (second term) and negative reward every timestep after we fall (first term). Suppose $\pi_E$ always takes action $a_1$ and never falls off the cliff (it is perfect). Suppose $\hat\pi$ takes action $a_2$ with probability $\varepsilon / 2$ in state $s_0$, and otherwise always takes $a_1$. First, note that $\pi_E$ never falls off the cliff, so $J(\pi_E) = 0$. On the other hand, $\hat\pi$ falls off the cliff in state $s_0$ with probability $\varepsilon / 2$, and stays away with probability $1 - \varepsilon / 2$. In the former case, it obtains reward $T$; in the latter case, it obtains reward $0$. Thus $J(\pi) = -\varepsilon T /2$, and the imitation gap is $J(\hat\pi) - J(\pi_E) = \varepsilon T / 2 \geq\Omega(\varepsilon T)$, as desired. All that remains is to construct $\hat f$ such that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium. We choose $\hat f = -r -1/2$ as in the previous cases; when translated from $\mathcal F$ to $\mathcal F_{Q_E}$, it defines an on-policy value of $-T$ for policies that stay away from the cliff, and a value of $T$ for policies that fall off the cliff (or have already fallen). To see that $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium, we must evaluate the three expressions $\sup_{f\in\mathcal F} U_3(\hat\pi, f)$, $U_3(\hat\pi, \hat f)$, and $\inf_{\pi\in\Pi} U_3(\pi, \hat f)$, and show they satisfy the inequality. First, note that after the first timestep, $\hat\pi$ and $\pi_E$ behave identically, whether we have fallen off the cliff or not. This helps us simplify the expression for $U_3(\hat\pi, f)$:
$$\begin{align*}U_3(\hat\pi, f) &= \frac{1}{T}\left[\mathop{\mathbb E}_{\tau\sim\hat\pi} \sum_{t=1}^T f(s_t, a_t) - \mathop{\mathbb E}_{\substack{\tau\sim\hat\pi\\a\sim \pi_E(s_t)}} \sum_{t=1}^T f(s_t, a)\right]\\&=\frac{1}{T}\mathop{\mathbb E}_{\tau\sim\hat\pi} \sum_{t=1}^T\left[ f(s_t, a_t) - \mathop{\mathbb E}_{a\sim \pi_E(s_t)}  f(s_t, a)\right]\\&=\frac{1}{T} \left[ \left(\frac{\varepsilon}{2}\right)(f(s_0, a_2) - f(s_0, a_1)) + \left(1 - \frac{\varepsilon}{2}\right)(0)\right]\\&=\frac{\varepsilon}{2T}(f(s_0, a_2) - f(s_0, a_1))\end{align*}$$
Clearly $\hat f$ is one of the maximizers of this expression, since it obtains the maximal possible value of $1/2$ on $(s_0, a_2)$ and minimal possible value of $-1/2$ on $(s_0, a_1)$. Thus$$\begin{align*}\sup_{f\in\mathcal F}U_3(\hat\pi, f) = U_3(\hat\pi, \hat f) =\frac{\varepsilon}{2T}\left(\frac{1}{2} - \left(-\frac{1}{2}\right)\right) = \frac{\varepsilon}{2T}\end{align*}$$Lastly, as in the reward case, the policy $\pi$ which minimizes $U_3(\pi, \hat f)$ is the expert policy $\pi_E$ which is penalized maximally at every timestep for staying away from the cliff. Thus, the two expectations cancel, and $\inf_{\pi\in\Pi} U_3(\pi, \hat f) = 0$; this makes sense, as according to $\hat f$, any policy is better than $\pi_E$, which never falls off the cliff. Putting all this together, the inequality defining an $\varepsilon$-approximate equilibrium is $-\frac{\varepsilon}{2} \left(1 - \frac{1}{T}\right)\leq \frac{\varepsilon}{2T}\leq \frac{\varepsilon}{2}$, which is clearly true, and thus $(\hat\pi, \hat f)$ is an $\varepsilon$-approximate equilibrium.