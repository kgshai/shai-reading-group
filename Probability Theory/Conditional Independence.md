Dependencies: [[Independence]]
## Introduction
We have derived a condition in terms of the joint distribution that indicates when two variables are independent. Consider however, three variables $X, Y, Z$, where $Z$ is the result of a first dice roll, $Y$ is the sum of the results of the first dice roll and a second dice roll, and $X$ is the sum of the results of the first dice roll and a third dice roll. 

Clearly $X$ and $Y$ are not independent: If $Y = 12$, then the first (and second) die must have rolled a 6, in which case $X\geq 7$. However, if $Y = 2$, then the first (and second) die must have rolled a 1, in which case $X$ could be as low as $2$. If we fix the result $Z=z$ of the first dice roll, however, we do have independent results, since the second and third dice rolls are independent. This situation is called **conditional independence**. It can be expressed with the same law as independence, except all variables (including the joint variable $X, Y$) are conditioned on $Z$:$$P_{X, Y\mid Z}(x, y\mid z) = P_{X\mid Z}(x\mid z)P_{Y\mid Z}(y\mid z)$$Thus, conditionally independent variables lead to a factored joint (conditional) distribution. Inspired by the case of independence, we wonder if a factored joint distribution also implies conditional independence. Suppose $P_{X, Y,Z} = f(x, z)g(y, z)h(z)$. For convenience, we let $F(z) = \sum_{x'} f(x', z)$ and $G(z)= \sum_{y'} f(y', z)$.$$\begin{align*}P_{X, Y, Z}(x, y, z) &= f(x, z) g(y, z) h(z)\\\Rightarrow\quad  P_{X, Y\mid Z}(x, y\mid z) &= \dfrac{P_{X, Y, Z}(x, y, z)}{\sum_{x', y'}P_{X, Y, Z}(x', y', z)} = \dfrac{f(x, z) g(y, z) h(z)}{\sum_{x', y'}f(x, z) g(y, z) h(z)} \\ & = \dfrac{f(x, z) g(y, z) h(z)}{h(z)\Big(\sum_{x'} f(x', z)\Big)\left(\sum_{y'} g(y, z)\right)} = \dfrac{f(x, z) g(y, z) }{F(z) G(z)}\\\Rightarrow\quad P_{X\mid Z}(x\mid z) &= \sum_{y} P_{X, Y\mid Z}(x, y\mid z) = \sum_{y}\dfrac{f(x, z) g(y, z) }{F(z) G(z)} = \dfrac{f(x, z) }{F(z)} \sum_y \dfrac{g(y, z)}{G(z)} = \dfrac{f(x, z)}{F(z)}\\\Rightarrow P_{Y\mid Z}(y\mid z) &= \sum_{x} P_{X, Y\mid Z}(x, y\mid z)=\sum_{x}\dfrac{f(x, z) g(y, z) }{F(z) G(z)} = \dfrac{g(y, z) }{G(Z)}\sum_x\dfrac{f(x, z)}{F(z)} = \dfrac{g(y, z)}{G(z)}\\\Rightarrow\quad P_{X, Y\mid Z}(x, y\mid z)&= P_{X\mid Z}(x\mid z) P_{Y\mid Z}(y\mid z) \end{align*}$$Mirroring [[Independence#^bc25af|the case]] of independence, two variables $X, Y$ are conditionally independent on $Z$ if and only if $P_{X, Y\mid Z}$ factors into terms $f(x, z)$, $g(y, z)$, and $h(z)$, and the distributions over $X\mid Z$ and $Y\mid Z$ are the normalizations of $f$ with respect to $x$ and $g$ with respect to $y$, respectively. Using the fact that $P_{X, Y, Z}(x, y, z) = P_{X,Y\mid Z}(x, y\mid z)P_Z(z)$, we see that $h$ is also related to the marginal over $Z$ via the normalization factors $F(z)$ and $G(z)$: $$P_Z(z) = \dfrac{P_{X, Y, Z}(x, y, z)}{P_{X, Y\mid Z}(x, y\mid z)} = \dfrac{f(x, z) g(y, z) h(z)}{\left(\dfrac{f(x, z)}{F(z)}\right)\left(\dfrac{g(y, z)}{G(z)}\right)} = F(z)G(z) h(z)$$
## Definition
Two random variables $X, Y$ are called **conditionally independent** on $Z$ when the following law holds: $$P_{X, Y\mid Z}(x, y\mid z) = P_{X\mid Z}(x\mid z)P_{Y\mid Z}(y\mid z)$$Or, equivalently, when the distribution $P_{X, Y\mid Z}$ can be factored as $P_{X, Y\mid Z}(x, y\mid z) = f(x, z)g(y, z)h(z)$, in which case the distributions over $X\mid Z$, $Y\mid Z$, and $Z$ are given by normalizations of the $f$,  $g$, and $h$ respectively: $$\begin{align*}P_{X\mid Z}(x\mid z) &= \dfrac{f(x, z)}{\sum_{x'} f(x', z)}\\ P_{Y\mid Z}(y\mid z) &= \dfrac{g(y, z)}{\sum_{y'} g(y', z)}\\ P_Z(z) &= h(z) \left(\sum_x f(x, z)\right) \left(\sum_y g(y, z)\right)\end{align*}$$