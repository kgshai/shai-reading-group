# Value Functions
Requirements: [[MDP]]
## Resources
- [Sutton & Barto 3.5 - 3.8](http://incompleteideas.net/book/RLbook2020.pdf)
- [ML@B Blog on MDPs](https://ml.berkeley.edu/blog/posts/mdps/)
- [Towards Data Science Series on MDPs](https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da)

## Introduction
Note that in the definition of the MDP, we would like to maximize expected cumulative reward *across entire trajectories*. To solve this problem computationally, it helps to know how the expected cumulative reward might change if we were starting in a different state, or if we took a different action. For the purpose of this analysis, we define the following functions:
1. We define $V^{\pi}(s)$ to be the expected reward-to-go when in state $s$ acting under policy $\pi$. 
2. We define $V^*(s) = \max_{\pi}V^{\pi}(s)$ to be the expected reward-to-go under the optimal policy.
3. We define $Q^{\pi}(s, a)$ to be the expected reward-to-go when taking action $a$ in state $s$ under policy $\pi$.
4. We define $Q^*(s, a)=\max_{\pi}Q^{\pi}(s, a)$ to be the expected reward-to-go under the optimal policy.
## Definition
We treat the simpler infinite-horizon case first, and then move to the finite-horizon case. In an  [[MDP#Infinite Horizon MDPs|Infinite Horizon MDP]], $$\begin{align*}Q^*(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s, a)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\\
Q^{\pi}(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s, a)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\\
V^*(s) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\\
V^{\pi}(s) &=  \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\end{align*}$$These expressions represent the expected total reward under the optimal policy and under policy $\pi$, respectively. To see why these may be useful, note that since our objective is simply the expected total reward, the optimal policy is given by$$\pi^*(s) = \mathop{\text{argmax}}_{a\in \mathcal A}\; Q^*(s, a)$$For an MDP with finite horizon $T\in\mathbb{N}$, we no longer have a single value function of each type. Rather, we have $T$ different value functions, one for each timestep, where $Q^*_t$, $Q^{\pi}_t$, $V^*_t$, and $V^{\pi}_t$ measure expected reward-to-go on timestep $t$ and afterwards:
$$\begin{align*}Q^*_t(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s, a)}\Bigg[\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\Bigg]\\Q^{\pi}_t(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s, a)}\Bigg[\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\Bigg]\\V^*_t(s) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s)}\Bigg[\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\Bigg] \\V^{\pi}_t(s) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s)}\Bigg[\sum_{t'=t}^{T}r(s_{t'}, a_{t'})\Bigg]\end{align*}$$
Once again, we have$$\pi^*_t(s) = \mathop{\text{argmax}}_{a\in\mathcal A}\; Q^*_t(s, a)$$And thus learning the $Q$-values under the optimal policy can be helpful in determining the optimal policy.
## Expectation Recurrence
Now recall the [[MDP#Decomposing Expectations |Expectation Decompositions]] for expectations over trajectory distributions:
$$\begin{align*}\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^{\pi}(\tau \mid s)} &= \mathop{\mathbb{E}}_{a_0\sim \pi(s)}\;\boxed{\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a_0)}\;\mathop{\mathbb{E}}_{a_1\sim \pi(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)} \dots}\\
\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^{\pi}(\tau \mid s, a)} &=\boxed{ \mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a)}\;\mathop{\mathbb{E}}_{a_1\sim \pi(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)}\;\mathop{\mathbb{E}}_{a_2\sim \pi(s_2)} \dots }\\\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s)} &= \mathop{\mathbb{E}}_{a_0\sim \pi^*(s)}\;\boxed{\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a_0)}\;\mathop{\mathbb{E}}_{a_1\sim \pi^*(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)} \dots}\\
\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s, a)} &=\boxed{ \mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a)}\;\mathop{\mathbb{E}}_{a_1\sim \pi^*(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)}\;\mathop{\mathbb{E}}_{a_2\sim \pi^*(s_2)} \dots}\end{align*}$$
Notice the similarity between the parts of the expressions we boxed. They allow us to write the following:
$$\begin{align*}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s)} &= \mathop{\mathbb{E}}_{a\sim\pi(s)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s, a)}\\\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s)} &= \mathop{\mathbb{E}}_{a\sim\pi^*(s)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s, a)}\end{align*}$$
(Note: In a Finite Horizon MDP, the boxed expressions start with the same expectations, but the expressions corresponding to $\mathcal{T}^{\pi}(\tau\mid s, a)$ and $\mathcal{T}^*(\tau\mid s, a)$ may not have the same number of expectations as those corresponding to $\mathcal{T}^{\pi}(\tau\mid s)$ and $\mathcal{T}^*(\tau\mid s)$. For convenience, we will assume we are dealing with an Infinite Horizon MDP. We will see that it is trivial to extend our results to Finite Horizon MDPs.) We can also see a similarity between the following boxed expressions:
$$\begin{align*}\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^{\pi}(\tau \mid s)} &=\boxed{ \mathop{\mathbb{E}}_{a_0\sim \pi(s)}\;\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a_0)}\;\mathop{\mathbb{E}}_{a_1\sim \pi(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)} \dots}\\
\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^{\pi}(\tau \mid s, a)} &= \mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a)}\;\boxed{\mathop{\mathbb{E}}_{a_1\sim \pi(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)}\;\mathop{\mathbb{E}}_{a_2\sim \pi(s_2)} \dots }\\\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s)} &= \boxed{\mathop{\mathbb{E}}_{a_0\sim \pi^*(s)}\;\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a_0)}\;\mathop{\mathbb{E}}_{a_1\sim \pi^*(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)} \dots}\\
\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s, a)} &= \mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a)}\;\boxed{\mathop{\mathbb{E}}_{a_1\sim \pi^*(s_1)}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)}\;\mathop{\mathbb{E}}_{a_2\sim \pi^*(s_2)} \dots}\end{align*}$$
which give rise to the following relations:
$$\begin{align*}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s, a)} &= \mathop{\mathbb{E}}_{s'\sim\mathcal{T}^{\pi}(s, a)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s')}\\\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s, a)} &= \mathop{\mathbb{E}}_{s'\sim\mathcal{T}^*(s, a)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s')}\end{align*}$$
These expectation recurrence relations will help us derive recurrence relations for $Q^{\pi}$, $Q^*$, $V^{\pi}$, and $V^*$, since they are defined using expectations over trajectories. But we are taking the expectation of total (discounted) reward, which is the objective that our optimal policy $\pi^*$ maximizes. This allows us to substitute $\underset{a_i\in A}{\max}$ for $\underset{a_i\sim \pi^*(s_i)}{\mathbb{E}}$ in our decomposition, altering one of our recurrences:$$\begin{align*}\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s)} &= \max_{a_0\in \mathcal A}\;\boxed{\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a_0)}\;\max_{a_1\in \mathcal A}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)} \dots}\\
\mathop{\mathbb{E}}_{\tau \sim \mathcal{T}^*(\tau \mid s, a)} &= \boxed{\mathop{\mathbb{E}}_{s_1\sim \mathcal{T}(s_1\mid s, a)}\;\max_{a_1\in \mathcal A}\;\mathop{\mathbb{E}}_{s_2\sim \mathcal{T}(s_2\mid s_1, a_1)}\;\max_{a_2\in \mathcal A} \dots}\\\Longrightarrow \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s)} &=\max_{a\in \mathcal A}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(s, a)}\end{align*}$$In conclusion, in Infinite-Horizon MDPs we can write
$$\begin{align*}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s, a)} &= \mathop{\mathbb{E}}_{s'\sim\mathcal{T}^{\pi}(s, a)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s')}\\\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s, a)} &= \mathop{\mathbb{E}}_{s'\sim\mathcal{T}^*(s, a)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s')}\\\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s)} &= \mathop{\mathbb{E}}_{a\sim\pi(s)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau\mid s, a)}\\\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau\mid s)}&=\max_{a\in A}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(s, a)}\end{align*}$$
We will use these to derive recurrence relations which relate $V$ to $Q$ and $Q$ to $V$. Putting these partial recurrence relations together, we can derive full recurrence relations which relate $V$ to $V$ and $Q$ to $Q$.

## Bellman Recurrence
^5e3551
Using the expectation recurrence relations we derived above (the last of which is only valid when we are applying it to the RL objective, as we are in $Q$ and $V$), we can derive the following partial recurrence relations:

|           | Partial Recurrence Relation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $Q^*$     | $$\begin{align*}Q^*(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s, a)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg] &\\&= \mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}}\;\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s')}\Bigg[\gamma^0r(s, a) + \sum_{t=0}^{\infty}\gamma^{t+1}r(s_t, a_t)\Bigg]& \\&= \mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}}\Bigg\{r(s, a) + \gamma\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s')}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\Bigg\}&\\&= r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}} V^*(s')&\end{align*}$$  |
| $Q^{\pi}$ | $$\begin{align*}Q^{\pi}(s, a) &= \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s)}\Bigg[\sum_{t=0}^{\infty}r(s_t, a_t)\Bigg]\\&=\mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\; \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s')}\Bigg[\gamma^0r(s, a) + \sum_{t=0}^{T}\gamma^{t+1}r(s_t, a_t)\Bigg]\\&= \mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\Bigg\{r(s, a) + \gamma\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s, a)}\Bigg[\sum_{t=0}^{\infty}\gamma^tr(s_t, a_t)\Bigg]\Bigg\}\\&=r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\;V^{\pi}(s')\end{align*} $$ |
| $V^*$     | $$\begin{align*}V^*(s) &= \bigg(\frac{1}{1-\gamma}\bigg)\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg]\\&=\bigg(\frac{1}{1-\gamma}\bigg) \max_{a\in A}\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s, a)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg] &\\&= \max_{a\in A}\bigg(\frac{1}{1-\gamma}\bigg)\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^*(\tau \mid s, a)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg]\\&= \max_{a\in A} Q^*(s, a)\end{align*}$$                                                                                                                                                 |
| $V^{\pi}$ | $$\begin{align*}V^{\pi}(s) &= \bigg(\frac{1}{1-\gamma}\bigg)\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg]\\&=\bigg(\frac{1}{1-\gamma}\bigg) \mathop{\mathbb{E}}_{a\sim \pi(a\mid s)}\;\mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s, a)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg] \\&=\mathop{\mathbb{E}}_{a\sim \pi(a\mid s)}\bigg(\frac{1}{1-\gamma}\bigg) \mathop{\mathbb{E}}_{\tau\sim\mathcal{T}^{\pi}(\tau \mid s, a)}\Bigg[\sum_{t=0}^{T}r(s_t, a_t)\Bigg]\\&=\mathop{\mathbb{E}}_{a\sim \pi(a\mid s)}\;Q^{\pi}(s, a)\end{align*}$$                                           |

^4e628e

We can plug these partial recurrence relations into one another to obtain the following full recurrence relations for Infinite Horizon MDPs:

|  | Full Recurrence Relation |
| --- | --- | 
|$Q^*$| $$\begin{align*}Q^*(s, a) &= r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}} V^*(s')\\&=r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}}\max_{a'\in A}Q^*(s', a')\end{align*}$$ |
| $Q^{\pi}$ | $$\begin{align*}Q^{\pi}(s, a) &=r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\;V^{\pi}(s')\\&=r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\;\mathop{\mathbb{E}}_{a'\sim \pi(a'\mid s')}\;Q^{\pi}(s', a')\end{align*} $$ |
| $V^*$ | $$\begin{align*}V^*(s) &=\max_{a\in A}Q^*(s, a)\\&= \max_{a\in A} \bigg\{r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T(s' \mid s,a)}} V^*(s')\bigg\}\end{align*}$$ |
| $V^{\pi}$ | $$\begin{align*}V^{\pi}(s) &=\mathop{\mathbb{E}}_{a\sim \pi(a\mid s)}\;Q^{\pi}(s, a)\\&=\mathop{\mathbb{E}}_{a\sim \pi(a\mid s)}\bigg\{r(s, a) + \gamma\mathop{\mathbb{E}}_{s'\sim \mathcal{T}(s' \mid s,a)}\;V^{\pi}(s')\bigg\}\end{align*}$$ |

These recurrence relations can be combined over multiple timesteps to produce the $n$-step Bellman recurrences:

|           | $n$-step Recurrence Relation                                                                                                                                                                                                                                                                                     |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $Q^*$     | $$Q^*(s_t, a_t) =\mathop{\mathbb{E}}_{s_{t+1}\sim \mathcal{T}(s_t,a_t)}\max_{a_{t+1}\in A}\dots\max_{a_{t+n}\in A}\left[\sum_{t'=t}^{t+n-1}r(s_{t'}, a_{t'}) + \gamma^n Q^*(s_{t+n}, a_{t+n})\right]$$                                                                                                           |
| $Q^{\pi}$ | $$Q^\pi(s_t, a_t) =\mathop{\mathbb{E}}_{\substack{s_{t+1}\sim \mathcal{T}(s_t,a_t)\\ a_{t+1}\sim\pi(s_{t+1})}}\dots\mathop{\mathbb{E}}_{\substack{s_{t+n}\sim \mathcal{T}(s_{t+n-1},a_{t+n-1})\\ a_{t+n}\sim\pi(s_{t+n})}}\left[\sum_{t'=t}^{t+n-1}r(s_{t'}, a_{t'}) + \gamma^n Q^\pi(s_{t+n}, a_{t+n})\right]$$ |
| $V^*$     | $$V^*(s_t) = \max_{a_t\in A}\mathop{\mathbb{E}}_{s_{t+1}\sim \mathcal{T}(s_t,a_t)}\dots\mathop{\mathbb{E}}_{s_{t+n}\sim \mathcal{T}(s_{t+n-1},a_{t+n-1})} \left[\sum_{t'=t}^{t+n-1} r(s_{t'}, a_{t'}) + \gamma^n V^*(s_{t+n})\right]$$                                                                           |
| $V^{\pi}$ | $$V^\pi(s_t) = \mathop{\mathbb{E}}_{\substack{a_t\sim\pi(s_t)\\ s_{t+1}\sim \mathcal{T}(s_t,a_t)}}\dots\mathop{\mathbb{E}}_{\substack{a_{t+n-1}\sim\pi(s_{t+n-1})\\ s_{t+n}\sim \mathcal{T}(s_{t+n-1},a_{t+n-1})}} \left[\sum_{t'=t}^{t+n-1} r(s_{t'}, a_{t'}) + \gamma^n V^\pi(s_{t+n})\right]$$                |

^24958b
