Dependencies: [[Probability Distribution]]
## Introduction
We often consider multiple random variables at once. It is helpful to extend the concept of a probability distribution to handle multiple random variables, and to answer questions that we frequently ask when dealing with multiple random variables. We showcase the concepts with two random variables, $X$ and $Y$. First of all, note that our random variables may be interrelated (e.g. if $X$ is the outcome of one dice roll, and $Y$ is the sum of the first dice roll and a second, separate roll), so we'll need a **joint probability distribution**, i.e. a probability distribution over the pair $(X, Y)$. We denote this distribution $P_{X, Y}(x, y)$ (which can be shortened to $P(x, y)$). Assuming we know the joint probability distribution, what distributions will $X$ and $Y$ follow individually? To answer this, we define a **marginal probability distribution**:$$P_X(x) = \sum_y P_{X, Y}(x, y)$$Here, the sum becomes an integral when $Y$ is continuous. Note that this distribution over $X$ does not assume a value for $Y$ (in our dice roll example, we are asking about the outcome of the first roll, not assuming any value for the sum of the two rolls). However, we would also like to know the distribution of $Y$ assuming $X$ takes a specific value $x$. This is given by the **conditional probability distribution**: $$P_{Y\mid X} (y\mid x) = \dfrac{P_{X, Y}(x, y)}{P_X(x)} = \dfrac{P_{X, Y}(x, y)}{\sum_{y'}P_{X, Y}(x, y')}$$where the PMF or PDF in the denominator is the marginal distribution. This value is only defined when $P_Y(y) > 0$: effectively, $Y$ must be able to take the value $y$ for us to condition on $Y$ taking this value.

## Definition
We define a distribution over two random variables using a **joint probability distribution** $P_{X, Y}(x, y)$. Given a joint probability distribution, we can define the **marginal probability distribution**, which gives the distribution over one random variable without any assumptions on the other, as $P_X(x) = \sum_y P_{X, Y}(x, y)$. We can also define the **conditional probability distribution**, which gives the distribution over one random variable assuming the other takes a specific value, via a quotient of the joint probability by the marginal probability:$$P_{Y\mid X} (y\mid x) = \dfrac{P_{X, Y}(x, y)}{P_X(x)} = \dfrac{P_{X, Y}(x, y)}{\sum_{y'}P_{X, Y}(x, y')}$$For greater than two random variables, the same laws can be applied by grouping (and ungrouping) multiple random variables into a single variable. For example, given four random variables $W, X, Y$, and $Z$ taking values in $\mathbb R$, we can obtain the conditional distribution of $W$ and $X$ given fixed values $Y = y$ and $Z = z$ by constructing variables $(W, X)$ and $(Y, Z)$ taking values in $\mathbb R^2$, and then applying the conditional distribution formula: $$P_{W, X \mid Y, Z}(w, x\mid y, z) = \dfrac{P_{W, X, Y, Z}(w, x, y, z)}{\sum_{y', z'}P_{W, X, Y, Z}(w, x, y', z')}$$
## Implications
One implication of the definitions above is that we can express the joint probability via the marginal probability and the conditional probability: $P_{X, Y}(x, y) = P_{Y\mid X}(y\mid x) P_X(x)$, which essentially says that the joint distribution is given by multiplying the distribution over $Y$ for each possible value of $X$ by the probability of that value of $X$. In our dice roll example, this states that the probability we obtain a first dice roll of $x$ and a dice roll sum of $y$ is equal to the probability that we first roll $x$, times the probability that we obtain the sum $y$ given that we rolled $x$. 

This approach can be expanded to $N$ random variables in order to factor a complex joint distribution in terms of simpler conditional distributions. This is known as the **chain rule of conditional probabilities**: $$P(X_1, \dots, X_n) = P(X_1) \prod_{i=2}^n P(X_i\mid X_1, \dots, X_n)$$where we have omitted subscripts for readability.