## Setup
Suppose we want to build a model to predict (a scalar) $Y$ based on some (scalar) data $X_1, X_2, \dots, X_m$, where there are $m$ different types of data. We could model this as a linear relationship:
$$Y = \beta_1X_1 +\beta_2X_2 + \dots + \beta_mX_m$$
We have $n > m$ samples, each consisting of a value $y_i$ and $m$ data values $x_{i1}, x_{i2}, \dots, x_{im}$. We can represent this problem using a **data matrix** $X$, **target vector** $\vec y$, and **parameter vector** $\vec\beta$:
$$X = \begin{pmatrix} x_{11} & x_{12} & \dots & x_{1m}\\x_{21} & x_{22} & \dots & x_{2m}\\\vdots & \vdots & \ddots & \vdots\\x_{n1} & x_{n2} & \dots & x_{nm}\end{pmatrix} \qquad\vec\beta= \begin{pmatrix}\beta_1 \\ \beta_2 \\\vdots \\ \beta_m\end{pmatrix} \qquad\vec y= \begin{pmatrix} y_1\\ y_2\\\vdots\\ y_n\end{pmatrix}$$
Then, our problem amounts to setting $\vec\beta$ so that $X\vec\beta$ is as close as possible to $\vec y$. We quantify "closeness" as the sum of squares:$$\min_{\vec \beta}  \lVert X\vec\beta - \vec y\rVert_2^2 $$Note this is equivalent to minimizing distance in $\mathbb{R}^n$ between $X\vec\beta$, the predicted values of $Y$, and the actual values $\vec y$. We can also view this geometrically: notice that $X\vec\beta$ creates arbitrary linear combinations of the columns of $X$ when we change the elements of $\beta$. Specifically, $$\begin{align*}\vec x_i &= \begin{pmatrix} x_{1i} \\ x_{2i} \\ \vdots \\ x_{ni}\end{pmatrix}\\ X &= \begin{pmatrix} \vec x_1 & \vec x_2 & \dots & \vec x_n\end{pmatrix}\\ X\beta &= \beta_1 \vec x_1 + \beta_2 \vec x_2 + \dots + \beta_m\vec x_m \approx \vec y\end{align*}$$We can picture each $\vec x_i$ and $\vec y$ as vectors in $\mathbb R^n$, and then our problem amounts to finding the vector in $\operatorname{span}\{\vec x_1, \vec x_2, \dots, \vec x_m\}$ that is closest to $\vec y$. This geometric picture is captured below.
![[Screenshot 2023-10-06 at 3.18.43 PM.png]]
## Standard Solution
The normal way to solve the least squares problem is to note that the optimal setting of $\vec \beta$ makes the error $\vec y - X\vec \beta$ orthogonal to the solution space $\operatorname{span}\{\vec x_1, \dots, \vec x_m\}$. This means that, for any data vector $\vec x_i$, we have $\vec x_i^T (\vec y - X\vec\beta) = 0$. We can wrap this into a single matrix equation:$$\begin{align*}&X^T(\vec y - X\vec\beta) = 0\\\implies & X^T\vec y - X^TX\vec\beta = 0\\\implies &\vec\beta=(X^TX)^{-1}X^T\vec y\end{align*}$$($X^TX$ has an inverse since it is an $m\times m$ matrix of rank $m$, assuming the data vectors are linearly independent, i.e. each of the $m$ features adds new information.)

